<!--
Hybrid AI Voice Assistant
- Client: hybrid_voice_assistant.html (this file)
- Server options included below (Python):
  1) faster-whisper local real-time WebSocket server (recommended for low-latency local setups)
  2) OpenAI Whisper (HTTP/stream) example (requires OpenAI API key) - higher cost, cloud

How it works (summary):
1) Client prefers a real-time Whisper WebSocket server if reachable (auto-detect)
2) If server available -> use WebSocket streaming (audio chunks) -> server returns partial/final transcriptions -> client updates UI + optional TTS
3) If server not available -> fallback to browser SpeechRecognition (webkit/SpeechRecognition) with optimizations
4) Waveform is driven by AudioContext (analyser) for real-time visual feedback
5) VAD (simple energy + silence timeout) batches audio to reduce bandwidth and lower latency
6) TTS uses SpeechSynthesis for Japanese (ja-JP)

USAGE:
- Save this file as hybrid_voice_assistant.html and open in Chrome (recommended)
- To use Whisper server functionality, run the Python server provided below and ensure ws://HOST:8765 is reachable

---------------------------------------------------------
CLIENT HTML / JS
---------------------------------------------------------><!doctype html>

<html lang="ja">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Hybrid AI Voice Assistant (Browser + Whisper)</title>
<style>
  :root{--accent:#00ffff;--accent-2:#00ffaa;--bg:#0f0f0f}
  html,body{height:100%;margin:0;background:var(--bg);color:#fff;font-family:Segoe UI,system-ui,Arial}
  canvas{position:fixed;inset:0;z-index:0}
  #ui{position:absolute;left:50%;bottom:5%;transform:translateX(-50%);z-index:10;width:min(980px,94vw)}
  #status-area{padding:14px 20px;border-radius:12px;background:rgba(0,0,0,0.45);box-shadow:0 0 20px #00ffff55;color:var(--accent);font-weight:700}
  #controls{display:flex;gap:12px;margin-top:10px;align-items:center}
  #messageInput{flex:1;padding:12px 14px;border-radius:10px;border:1px solid rgba(0,255,255,0.15);background:rgba(255,255,255,0.03);color:#fff;font-size:16px}
  button{padding:10px 14px;border-radius:10px;border:none;cursor:pointer;font-weight:700}
  #micBtn{background:var(--accent-2);color:#000}
  #resetBtn{background:var(--accent);color:#000}
  #modeIndicator{padding:8px 10px;border-radius:8px;background:#00000044;font-size:0.9rem}
  #subtext{margin-top:8px;color:#bfeeff}
  .active{box-shadow:0 0 20px #ff5555}
  #transcript{margin-top:12px;padding:12px;border-radius:10px;background:rgba(255,255,255,0.02);min-height:48px;font-size:16px}
</style>
</head>
<body>
<canvas id="waveCanvas"></canvas>
<div id="ui">
  <div id="status-area">Initializing...</div>
  <div id="controls">
    <input id="messageInput" placeholder="Ë©±„Åó„Åã„Åë„Å¶„Åè„Å†„Åï„ÅÑ...">
    <button id="micBtn">üé§ Start</button>
    <button id="resetBtn">„É™„Çª„ÉÉ„Éà</button>
    <div id="modeIndicator">--</div>
  </div>
  <div id="subtext">„É™„Ç¢„É´„Çø„Ç§„É†Â≠óÂπï„ÉªËá™ÂãïÂàáÊõøÔºà„É≠„Éº„Ç´„É´Whisper„Çµ„Éº„Éê„Éº ‚áÑ „Éñ„É©„Ç¶„Ç∂Ë™çË≠òÔºâ</div>
  <div id="transcript"></div>
</div><script>
/* ---------- Config ---------- */
const WHISPER_WS_URL = 'ws://localhost:8765'; // change if server is remote
const VAD_SILENCE_MS = 350; // silence timeout to commit chunk
const AUDIO_CHUNK_MS = 300; // chunk length sent to server (ms)
const ENABLE_FALLBACK = true; // use browser SpeechRecognition if server unreachable

/* ---------- DOM ---------- */
const statusArea = document.getElementById('status-area');
const micBtn = document.getElementById('micBtn');
const resetBtn = document.getElementById('resetBtn');
const input = document.getElementById('messageInput');
const modeIndicator = document.getElementById('modeIndicator');
const transcriptBox = document.getElementById('transcript');

/* ---------- Audio / Waveform ---------- */
const canvas = document.getElementById('waveCanvas');
const ctx = canvas.getContext('2d');
function resizeCanvas(){canvas.width = innerWidth; canvas.height = innerHeight}
window.addEventListener('resize', resizeCanvas); resizeCanvas();

let audioContext, analyser, mediaStream, sourceNode, processorNode;
let isRecording = false;
let ws=null, wsAvailable=false;
let useServer=false;
let recognition=null; // browser fallback

/* VAD helper */
let lastSpokenTime = 0;
let vadSilenceTimer = null;
function nowMs(){return performance.now()}

/* Simple energy VAD */
function isLoud(buffer, threshold=0.01){
  let sum=0; for(let i=0;i<buffer.length;i++){ let v=buffer[i]; sum += v*v }
  let rms = Math.sqrt(sum / buffer.length);
  return rms > threshold;
}

/* Waveform draw */
let waveformData = new Float32Array(1024);
function drawWave(){
  ctx.clearRect(0,0,canvas.width,canvas.height);
  if(!analyser) return requestAnimationFrame(drawWave);
  analyser.getFloatTimeDomainData(waveformData);

  ctx.lineWidth = 2;
  const midY = canvas.height * 0.55;
  ctx.beginPath();
  const step = canvas.width / waveformData.length;
  for(let i=0;i<waveformData.length;i++){
    const x = i * step; const y = midY + waveformData[i]*midY*0.8;
    if(i===0) ctx.moveTo(x,y); else ctx.lineTo(x,y);
  }
  const gradient = ctx.createLinearGradient(0,0,canvas.width,canvas.height);
  gradient.addColorStop(0,'rgba(0,255,255,0.1)'); gradient.addColorStop(1,'rgba(0,170,255,0.15)');
  ctx.strokeStyle = 'rgba(0,255,255,0.8)';
  ctx.stroke();
  requestAnimationFrame(drawWave);
}

/* ---------- WebSocket (Whisper) client ---------- */
function connectWS(){
  status('Connecting to Whisper server...');
  ws = new WebSocket(WHISPER_WS_URL);
  ws.binaryType = 'arraybuffer';
  ws.onopen = ()=>{ wsAvailable=true; status('Whisper server connected'); modeIndicator.textContent='mode: server (Whisper)'; useServer=true };
  ws.onmessage = (ev)=>{
    // assume text messages are partial/final transcripts
    const text = typeof ev.data === 'string' ? ev.data : new TextDecoder().decode(ev.data);
    // server may send JSON: {type:'partial'|'final', text:'...'}
    try{
      const j = JSON.parse(text);
      if(j.type === 'partial'){
        input.value = j.text; transcriptBox.textContent = j.text;
      } else if(j.type==='final'){
        input.value = j.text; transcriptBox.textContent = j.text; speak(j.text);
      } else {
        input.value = j.text; transcriptBox.textContent = j.text;
      }
    }catch(e){
      // plain text fallback
      input.value = text; transcriptBox.textContent = text; speak(text);
    }
  };
  ws.onerror = (e)=>{ console.warn('ws err',e); wsAvailable=false; useServer=false; if(ENABLE_FALLBACK) status('Whisper server unreachable. Will use browser STT fallback'); };
  ws.onclose = ()=>{ wsAvailable=false; useServer=false; if(isRecording) stopRecording(); status('Whisper server closed'); modeIndicator.textContent='mode: fallback'; };
}

/* ---------- Audio capture & streaming ---------- */
async function startRecording(){
  if(isRecording) return;
  isRecording = true; micBtn.textContent='‚ñ† Stop'; micBtn.classList.add('active');
  try{
    mediaStream = await navigator.mediaDevices.getUserMedia({audio:true});
  }catch(err){ status('„Éû„Ç§„ÇØ„ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü'); console.error(err); return }

  audioContext = new (window.AudioContext || window.webkitAudioContext)({sampleRate: 16000});
  sourceNode = audioContext.createMediaStreamSource(mediaStream);
  analyser = audioContext.createAnalyser(); analyser.fftSize = 2048;
  sourceNode.connect(analyser);
  // use ScriptProcessor for simplicity (deprecated but widely supported). Replace with AudioWorklet for production.
  processorNode = audioContext.createScriptProcessor(4096,1,1);
  sourceNode.connect(processorNode); processorNode.connect(audioContext.destination);

  const bufferQueue = [];
  let chunkMillis = AUDIO_CHUNK_MS;
  let sampleRate = audioContext.sampleRate;
  let samplesPerChunk = Math.floor(sampleRate * (chunkMillis/1000));

  processorNode.onaudioprocess = (evt)=>{
    const chData = evt.inputBuffer.getChannelData(0);
    // copy
    const chunk = new Float32Array(chData.length); chunk.set(chData);
    // push to queue
    bufferQueue.push(chunk);

    // VAD
    if(isLoud(chData)){
      lastSpokenTime = nowMs();
      if(vadSilenceTimer) clearTimeout(vadSilenceTimer);
    }

    // every samplesPerChunk samples -> prepare PCM16 and send
    // we collect enough from bufferQueue
    let totalSamples = bufferQueue.reduce((a,b)=>a+b.length,0);
    if(totalSamples >= samplesPerChunk){
      // concat
      const out = new Float32Array(samplesPerChunk);
      let offset=0;
      while(offset < samplesPerChunk){
        const piece = bufferQueue.shift();
        const needed = Math.min(piece.length, samplesPerChunk - offset);
        out.set(piece.subarray(0,needed), offset);
        if(needed < piece.length){
          // push remaining back
          bufferQueue.unshift(piece.subarray(needed));
        }
        offset += needed;
      }
      // silence detection and send
      const loud = isLoud(out);
      if(useServer && ws && ws.readyState===1){
        // convert Float32 to 16-bit PCM
        const pcm16 = floatTo16BitPCM(out);
        // send as raw ArrayBuffer or framed message; server must expect raw PCM16
        ws.send(pcm16.buffer);
      } else if(!useServer && ENABLE_FALLBACK){
        // do nothing here: browser fallback uses SpeechRecognition API
      }
    }

    // if silence detected for a bit, commit final (only when using server)
    if(useServer){
      if(nowMs() - lastSpokenTime > VAD_SILENCE_MS){
        // send a special commit signal if desired
        if(ws && ws.readyState===1){ try{ ws.send(JSON.stringify({type:'commit'})) }catch(e){} }
        lastSpokenTime = nowMs();
      }
    }
  };

  drawWave();
  status('Recording...');

  // start fallback SpeechRecognition if needed and server unavailable
  if(ENABLE_FALLBACK && !useServer) startBrowserRecognition();
}

function stopRecording(){
  if(!isRecording) return;
  isRecording=false; micBtn.textContent='üé§ Start'; micBtn.classList.remove('active');
  try{ processorNode && processorNode.disconnect(); sourceNode && sourceNode.disconnect(); analyser && (analyser.disconnect && analyser.disconnect()); }
  catch(e){}
  mediaStream && mediaStream.getTracks().forEach(t=>t.stop());
  audioContext && audioContext.close(); audioContext=null; analyser=null; processorNode=null; sourceNode=null;
  status('Stopped');
  stopBrowserRecognition();
}

function floatTo16BitPCM(float32Array){
  const l = float32Array.length; const buf = new ArrayBuffer(l*2); const view = new DataView(buf);
  let offset=0; for(let i=0;i<l;i++){ let s=Math.max(-1,Math.min(1,float32Array[i])); view.setInt16(offset, s<0? s*0x8000 : s*0x7FFF, true); offset+=2 }
  return new Int16Array(buf);
}

/* ---------- Browser SpeechRecognition fallback ---------- */
function startBrowserRecognition(){
  if(!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)){
    status('„Éñ„É©„Ç¶„Ç∂Èü≥Â£∞Ë™çË≠ò„ÅåÊú™ÂØæÂøú„Åß„Åô'); return; }
  const SpeechRec = window.SpeechRecognition || window.webkitSpeechRecognition;
  recognition = new SpeechRec(); recognition.lang='ja-JP'; recognition.interimResults=true; recognition.continuous=true;
  recognition.onstart = ()=>{ status('Browser STT: listening (fallback)'); modeIndicator.textContent='mode: browser STT'; }
  recognition.onresult = (ev)=>{
    let interim=''; let final='';
    for(let i=0;i<ev.results.length;i++){ const t=ev.results[i][0].transcript; ev.results[i].isFinal? final+=t: interim+=t }
    input.value = final || interim; transcriptBox.textContent = input.value;
    if(final) speak(final);
  };
  recognition.onerror = (e)=>{ console.warn('recog err',e); };
  recognition.onend = ()=>{ console.log('rec end'); };
  recognition.start();
}
function stopBrowserRecognition(){ if(recognition){ try{ recognition.stop() }catch(e){} recognition=null } }
<!--
/* ---------- TTS ---------- */
const synth = window.speechSynthesis;
let lastSpokenText = '';
function speak(text){ if(!text || text===lastSpokenText) return; lastSpokenText = text; if(synth.speaking) synth.cancel(); const u = new SpeechSynthesisUtterance(text); u.lang='ja-JP'; u.rate=1.0; u.onstart=()=>{ status('Speaking...'); }; u.onend=()=>{ status('Idle'); }; synth.speak(u); }

/* ---------- UI helpers ---------- */
function status(msg){ statusArea.textContent = msg }
function drawWave(){ if(!analyser) return requestAnimationFrame(drawWave); analyser.getFloatTimeDomainData(waveformData); /* placeholder, will be overwritten by actual analyser reference*/ }

/* ---------- Controls ---------- */
micBtn.addEventListener('click', async ()=>{
  if(!isRecording){
    // try connect to server first
    try{ connectWS(); await new Promise(r=>setTimeout(r,250)); }catch(e){}
    if(wsAvailable) useServer=true; else useServer=false;
    startRecording();
  } else {
    stopRecording();
  }
});
resetBtn.addEventListener('click', ()=>{ input.value=''; transcriptBox.textContent=''; if(synth.speaking) synth.cancel(); status('Reset'); });


  

/* ---------- Start-up ---------- */
(function init(){
  status('Initializing audio...');
  // start visual loop
  (function loopCanvas(){
    if(analyser){
      const bufferLen = analyser.fftSize; const data = new Float32Array(bufferLen);
      analyser.getFloatTimeDomainData(data);
      ctx.clearRect(0,0,canvas.width,canvas.height);
      ctx.beginPath();
      const mid = canvas.height*0.55; const step = canvas.width / bufferLen;
      for(let i=0;i<bufferLen;i++){ const x=i*step; const y=mid + data[i]*mid*0.9; if(i===0)ctx.moveTo(x,y);else ctx.lineTo(x,y) }
      ctx.strokeStyle='rgba(0,230,255,0.9)'; ctx.lineWidth=2; ctx.stroke();
    }
    requestAnimationFrame(loopCanvas);
  })();

  // try ping server quickly
  try{ const p = new WebSocket(WHISPER_WS_URL); p.onopen=()=>{ p.close(); status('Whisper server reachable'); modeIndicator.textContent='mode: server (available)'; }; p.onerror=()=>{ modeIndicator.textContent='mode: fallback (no server)'; status('Whisper server not reachable - will use browser STT'); }; }
  catch(e){ modeIndicator.textContent='mode: fallback'; status('ready'); }
})();

</script></body>
</html><!--
---------------------------------------------------------
Python Server Option A: faster-whisper WebSocket server (local)
- Install: pip install faster-whisper websockets soundfile numpy
- This server expects raw PCM16LE frames (Int16Array) from client.
- It sends JSON messages: {type:'partial', text:'...'} during streaming and {type:'final', text:'...'} when committed.

Save as whisper_ws_server.py and run: python whisper_ws_server.py
---------------------------------------

whisper_ws_server.py

""" Simple WebSocket wrapper around faster-whisper for low-latency streaming. This is a minimal example and not production hardened. """

import asyncio import json import websockets import numpy as np from faster_whisper import WhisperModel

MODEL_SIZE = "small"  # choose 'tiny', 'base', 'small', 'medium', 'large' SAMPLE_RATE = 16000

model = WhisperModel(MODEL_SIZE, device="cpu")

naive buffer per connection

connections = {}

async def handler(ws, path): print('client connected') buffer = np.zeros((0,), dtype=np.int16) partial_text = "" try: async for message in ws: # Expect either binary PCM16LE or JSON control messages if isinstance(message, bytes): # append received PCM16LE bytes arr = np.frombuffer(message, dtype=np.int16) buffer = np.concatenate([buffer, arr]) # when buffer size reaches ~0.5s, run a quick transcription if len(buffer) > SAMPLE_RATE * 0.25: # convert to float32 -1..1 audio_float = buffer.astype(np.float32) / 32768.0 # faster-whisper supports transcribe on numpy arrays segments, _ = model.transcribe(audio_float, beam_size=1, language='ja', vad_filter=True) text = ''.join([seg.text for seg in segments]) # send as partial try: await ws.send(json.dumps({"type":"partial","text":text})) except Exception as e: print('ws send partial err',e) # keep a short tail to allow overlapping # keep last 0.1s tail = int(SAMPLE_RATE * 0.1) if len(buffer) > tail: buffer = buffer[-tail:] else: # control JSON try: j = json.loads(message) if j.get('type') == 'commit': # finalise current buffer if buffer.size>0: audio_float = buffer.astype(np.float32) / 32768.0 segments, _ = model.transcribe(audio_float, beam_size=1, language='ja') text = ''.join([seg.text for seg in segments]) await ws.send(json.dumps({"type":"final","text":text})) buffer = np.zeros((0,), dtype=np.int16) except Exception as e: print('control parse err', e) except websockets.exceptions.ConnectionClosed: print('client disconnected')

async def main(): async with websockets.serve(handler, '0.0.0.0', 8765, max_size=None, max_queue=None): print('Whisper WebSocket server running on ws://0.0.0.0:8765') await asyncio.Future()

if name == 'main': asyncio.run(main())

<!--
---------------------------------------------------------
Python Server Option B: OpenAI Whisper (HTTP) - example sketch
- This is a conceptual example. OpenAI's exact streaming endpoints may differ; check their docs.
- Use this if you want cloud-hosted transcription (requires API key and network connectivity).
--
openai_whisper_stream.py (concept sketch)

import requests

This is a placeholder - OpenAI streaming examples require specific setup (see OpenAI docs)

Example: send audio file and get transcript

url = 'https://api.openai.com/v1/audio/transcriptions'

headers = {'Authorization': f'Bearer {OPENAI_API_KEY}'}

resp = requests.post(url, headers=headers, files={'file': open('chunk.wav','rb')}, data={'model':'whisper-1','language':'ja'})

print(resp.json())

For true low-latency streaming you'd use websockets or gRPC-based streaming on the server side.---->


------------------------------------------------------- ! >

